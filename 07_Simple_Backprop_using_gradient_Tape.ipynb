{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgcortes/KerasTensor/blob/master/07_Simple_Backprop_using_gradient_Tape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de743a9b"
      },
      "source": [
        "# Automatic Differentiation and GradientTape"
      ],
      "id": "de743a9b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d087734"
      },
      "source": [
        "* **Backpropagation** algorithm is used heavily in neural networks to update the model's parameters.\n",
        "* The algorithm works by continuously moving backwards in the network, finding the partial derivatives of the loss function w.r.t. the model's parameters, and then performing the parameter updates.\n",
        "* A key task in backpropagation is to first find out the gradient values for each trainable parameter of the model.\n",
        "* We know that **Backpropagation algorithm uses Chain rule to find the partial derivatives.**\n",
        "* But to derive and define the gradient calculation for each parameter on our own can be tedious, there being so many parameters.\n",
        "* However, when using modern Deep Learning frameworks such as Tensorflow, PyTorch or MXNet, we generally don't have to worry about calculating these gradients manually. It’s done automatically for us.\n",
        "\n",
        "\n",
        "<img src='https://learnopencv.com/wp-content/uploads/2022/01/c4_02_GradientTape_Poster.png' width=400 align='center'>"
      ],
      "id": "8d087734"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml4sLrsFeif0"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "* [1 Objective](#1-Objective)\n",
        "* [2 GradientTape](#2-GradientTape)\n",
        "    * [2.1 Auto-differentiation Example](#2.1-Auto-differentiation-Example)\n",
        "* [3 GradientTape Arguments](#3-GradientTape-Arguments)\n",
        "* [4 Back-Progagation Examle](#4-Multivariate-Linear-Regression)\n",
        "    * [4.1 Problem Setup](#4.1-Problem-Setup)\n",
        "    * [4.2 Sigmoid Function](#4.2-Sigmoid-Function)\n",
        "    * [4.3 Binary Cross-Entropy](#4.3-Binary-Cross\\-Entropy)\n",
        "    * [4.4 Solving Manually](#4.4-Solving-Manually)\n",
        "* [5 Direct Implementation using GradientTape](#5-Direct-Implementation-using-GradientTape)"
      ],
      "id": "Ml4sLrsFeif0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae0245ad"
      },
      "source": [
        "## 1 Objective"
      ],
      "id": "ae0245ad"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6ce48d8"
      },
      "source": [
        "In this notebook, we'll study the `GradientTape API` provided by TensorFlow, and see for ourselves how easy it is to perform automatic differentiation.\n",
        "    \n",
        "1. First, we will explain what `GradientTape` is and show through an example how it can be used for automatic differentiation.\n",
        "2. Next, we'll briefly discuss the different GradientTape arguments.\n",
        "3. Finally, we take a backpropagation example to demonstrate the effectiveness of the GradientTape API.\n",
        "4. In the section **Backpropagation Example**, we will start by explaining the example problem we are trying to solve, then go over the two steps in backpropagation as well as defining  the various mathematical equations involved in backpropagation. After that, we shall explore three different ways to solve the example problem.\n",
        "    1. ***Manually:*** In this section, we solve the problem manually, by first deriving the derivative equations and defining the gradient functions for individual functions. We then find out the gradient value for the respective derivatives involved in the chain rule and go on to solve its equations.\n",
        "    2. ***Using GradientTape:*** Next you’ll  see how with the help of GradientTape we quickly find the gradients for all the individual functions involved, without defining the gradient functions. Then use these gradient values to solve the chain rule equations.\n",
        "    3. ***Direct method:*** Finally, we see a more general and direct way of using GradientTape. Here, we only need to focus on executing the forward pass correctly, and at the end, we get the final gradients w.r.t. the variables with just one method call. You needn’t worry here about implementing the chain rule. The GradientTape will handle it internally.\n"
      ],
      "id": "a6ce48d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffb06da6"
      },
      "source": [
        "## 2 GradientTape"
      ],
      "id": "ffb06da6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "738c837e"
      },
      "source": [
        "From Tensorflow: \n",
        "\n",
        "> ***TensorFlow provides the tf.GradientTape API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually tf.Variables. TensorFlow \"records\" relevant operations executed inside the context of a tf.GradientTape onto a \"tape\". <br>\n",
        "TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using reverse mode differentiation.\"***"
      ],
      "id": "738c837e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e28c1ab9"
      },
      "source": [
        "### 2.1 Auto-differentiation Example"
      ],
      "id": "e28c1ab9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:14.966277Z",
          "start_time": "2021-12-20T16:56:10.966826Z"
        },
        "collapsed": true,
        "id": "6cc5f53e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ],
      "id": "6cc5f53e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:16.887020Z",
          "start_time": "2021-12-20T16:56:14.969279Z"
        },
        "id": "05335fda"
      },
      "outputs": [],
      "source": [
        "# Declare a tensorflow Variable\n",
        "\n",
        "x = tf.Variable(2.0)"
      ],
      "id": "05335fda"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f397954a"
      },
      "source": [
        "Within the **`with tf.GradientTape() as tape`** context manager we'll perform some operations.\n",
        "\n",
        "\n",
        "> **\"To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass. Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to compute gradients.**"
      ],
      "id": "f397954a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:16.918019Z",
          "start_time": "2021-12-20T16:56:16.891037Z"
        },
        "collapsed": true,
        "id": "fbfc8c09"
      },
      "outputs": [],
      "source": [
        "with tf.GradientTape() as tape:\n",
        "    # perform squaring operation\n",
        "    y = x ** 2\n",
        "    \n",
        "    # Now that GradientTape has recorded the operation  \n",
        "    # we can calculate the gradient of the operation i.e. dy/dx\n",
        "    \n",
        "\n",
        "# Note: We are now outside the GradientTape context\n",
        "# Gradient calculations and updates need to be performed\n",
        "# outside the GradientTape context, or these operations will be\n",
        "# recorded on the tape as well, and increased memory usage.\n",
        "\n",
        "\n",
        "dy_dx = tape.gradient(y, x)"
      ],
      "id": "fbfc8c09"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d63ede7"
      },
      "source": [
        "* The `.gradient(...)` is responsible for actually calculating the gradients.\n",
        "<br><br>\n",
        "\n",
        "**`.gradient(...)` method's signature**\n",
        "\n",
        "```python\n",
        "Signature:\n",
        "tape.gradient(\n",
        "    target,\n",
        "    sources,\n",
        "    output_gradients=None,\n",
        "    unconnected_gradients=<UnconnectedGradients.NONE: 'none'>,\n",
        ")\n",
        "\n",
        "Docstring:\n",
        "Computes the gradient using operations recorded in context of this tape.\n",
        "\n",
        "Note: Unless you set `persistent=True` a GradientTape can only be used to\n",
        "compute one set of gradients (or jacobians).\n",
        "\n",
        "Args:\n",
        "  target: a list or nested structure of Tensors or Variables to be\n",
        "    differentiated.\n",
        "  sources: a list or nested structure of Tensors or Variables. `target`\n",
        "    will be differentiated against elements in `sources`.\n",
        "  output_gradients: a list of gradients, one for each element of\n",
        "    target. Defaults to None.\n",
        "  unconnected_gradients: a value which can either hold 'none' or 'zero' and\n",
        "    alters the value which will be returned if the target and sources are\n",
        "    unconnected. The possible values and effects are detailed in\n",
        "    'UnconnectedGradients' and it defaults to 'none'.\n",
        "\n",
        "Returns:\n",
        "  a list or nested structure of Tensors (or IndexedSlices, or None),\n",
        "  one for each element in `sources`. Returned structure is the same as\n",
        "  the structure of `sources`.\n",
        "```"
      ],
      "id": "8d63ede7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:16.933052Z",
          "start_time": "2021-12-20T16:56:16.921025Z"
        },
        "id": "5095813f"
      },
      "outputs": [],
      "source": [
        "# Gradient value\n",
        "\n",
        "print(dy_dx.numpy())"
      ],
      "id": "5095813f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8360ab7d"
      },
      "source": [
        "We got an output of 4.0.<br>\n",
        "Now, let's calculate and check it manually:\n",
        "\n",
        "**Derivative:**<br>\n",
        "\n",
        "$$y = x^{2} $$\n",
        "\n",
        "$$\\frac{dy}{dx} = \\frac{d (x^{2})}{dx}$$ <br><br>\n",
        "\n",
        "Derivative of: $\\mathbf{x^{n}=n*x^{n-1}}$, Therefore,\n",
        "\n",
        "$$\\frac{dy}{dx} = 2 * \\frac{d (x)}{dx}$$\n",
        "\n",
        "$$\\frac{dy}{dx} = 2 * x $$\n",
        "\n",
        "$$\\frac{dy}{dx} = 2 * 2 $$\n",
        "\n",
        "$$\\frac{dy}{dx} = 4 $$\n",
        "\n"
      ],
      "id": "8360ab7d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb3da551"
      },
      "source": [
        "## 3 GradientTape Arguments"
      ],
      "id": "bb3da551"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e78c65e"
      },
      "source": [
        "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />\n",
        "\n",
        "The `tf.GradientTape` class takes in 2 parameters, which are as follows:\n",
        "\n",
        "* **watch_accessed_variables:** (Boolean, Default: `True`) Controls whether the tape will automatically `watch` any (trainable) variables that are accessed while the tape is active. This means gradients can be requested from any result computed in the tape derived from reading a trainable Variable. If False, users must explicitly `watch` any Variables they want to request gradients from.\n",
        "* **persistent:** (Boolean, Default: `False`) Controls whether or not to create a persistent gradient tape. This should be used when you need to compute more than one set of gradients.\n",
        "    \n",
        "\n",
        "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />"
      ],
      "id": "1e78c65e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9fca55f"
      },
      "source": [
        "## 4 Backpropagation Example"
      ],
      "id": "b9fca55f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2cb6ae6"
      },
      "source": [
        "### 4.1 Problem Setup"
      ],
      "id": "b2cb6ae6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fbf5a8d"
      },
      "source": [
        "**Our goal here is to show the effectiveness of `GradientTape` API.<br>\n",
        "We'll demonstrate how you can use GradientTape API to minimize a loss function, using the backpropagation algorithm**\n",
        "\n",
        "We will take an example that will include operations closely resembling a deep learning pipeline.\n",
        "<img src='https://learnopencv.com/wp-content/uploads/2022/01/c4_02_weight_update.png' width=600 align='center'>"
      ],
      "id": "5fbf5a8d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1b800cc"
      },
      "source": [
        "**Let's define our equations:**\n",
        "\n",
        "\n",
        "$$ A_{1} = x_{1} * w_{1} + x_{2} * w_{2} + b$$\n",
        "\n",
        "$$ A_{2} = sigmoid(A_{1})$$\n",
        "\n",
        "$$ A_{3} = J(A_{2}, Y)$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "___\n",
        "\n",
        "1. In the above equations, we have:\n",
        "    1. 3 scalar variables: $\\bf{w_{1}, w_{2}}$ and $\\bf{b}$\n",
        "    2. 2 input scalar constants $\\bf{x_{1}}$ and $\\bf{x_{2}}$\n",
        "2. $Y$ represents the constant scalar ground-truth class.\n",
        "3. $A_{1}$ computes the weighted addition of the inputs.\n",
        "4. $A_{2}$ applies the Sigmoid function ($\\sigma(x)$) to $A_{1}$.\n",
        "5. $A_{3}$ applies the Binary cross-entropy loss function ($J(...))$) to calculate the loss between $A_{2}$ and $Y$."
      ],
      "id": "f1b800cc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48947a17"
      },
      "source": [
        "The Backpropagation algorithm involves two steps:\n",
        "1. Using the **Chain rule** to get the gradient of loss with respect to the variables.\n",
        "2. Updating the variables using the **weight-update rule.**\n",
        "\n",
        "\n",
        "\n",
        "We will first use the Chain rule to find out gradients: $A_{3}'(w_{1})$, $A_{3}'(w_{2})$ and $A_{3}'(b)$. Then update the variables using the weight-update rule to show the decrease in loss.\n",
        "\n",
        "\n",
        "\n",
        "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />\n",
        "\n",
        "\n",
        "**The Chain rule from the above-mentioned equations are as follows:**\n",
        "\n",
        "1. $A_{3}'(w_{1})$ is given by:\n",
        "$$\\frac{\\partial (A_{3})}{\\partial (w_{1})} = \\frac{\\partial (A_{3})}{\\partial (A_{2})} \\;*\\; \\frac{\\partial (A_{2})}{\\partial (A_{1})} \\;*\\; \\frac{\\partial (A_{1})}{\\partial (w_{1})}$$\n",
        "\n",
        "\n",
        "2. $A_{3}'(w_{2})$ is given by:\n",
        "$$\\frac{\\partial (A_{3})}{\\partial (w_{2})} = \\frac{\\partial (A_{3})}{\\partial (A_{2})} \\;*\\; \\frac{\\partial (A_{2})}{\\partial (A_{1})}\\;*\\; \\frac{\\partial (A_{1})}{\\partial (w_{2})}$$\n",
        "\n",
        "\n",
        "3. $A_{3}'(b)$ is given by:\n",
        "$$\\frac{\\partial (A_{3})}{\\partial (b)} = \\frac{\\partial (A_{3})}{\\partial (A_{2})} \\;*\\; \\frac{\\partial (A_{2})}{\\partial (A_{1})}\\;*\\; \\frac{\\partial (A_{1})}{\\partial (b)}$$\n",
        "\n",
        "\n",
        "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />\n",
        "\n",
        "**Weight-Update Rule**\n",
        "\n",
        "After calculating the gradients, you can update the weights using the following equations:\n",
        "\n",
        "$$w_{1} \\leftarrow w_{1} - \\gamma \\frac{\\partial A_{3}}{\\partial w_{1}}$$\n",
        "\n",
        "$$w_{2} \\leftarrow w_{2} - \\gamma \\frac{\\partial A_{3}}{\\partial w_{2}}$$\n",
        "\n",
        "\n",
        "$$b \\leftarrow b - \\gamma \\frac{\\partial A_{3}}{\\partial b}$$\n",
        "\n",
        "Where. $\\gamma$ represents the learning rate."
      ],
      "id": "48947a17"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_style": "center",
        "heading_collapsed": true,
        "id": "3aa4cf9b"
      },
      "source": [
        "###  4.2 Sigmoid Function\n",
        "\n",
        "**The Sigmoid function and its derivative is given by:**\n",
        "\n",
        "Let $\\mathbf{y' = \\sigma(z)}$, then:\n",
        "$$y' = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "**Derivative of sigmoid w.r.t. its input.**\n",
        "\n",
        "$$\\frac{\\partial y'}{\\partial z}  = \\sigma(z)(1 - \\sigma(z)) = y'(1-y')$$"
      ],
      "id": "3aa4cf9b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe8d711f"
      },
      "source": [
        "### 4.3 Binary Cross-Entropy"
      ],
      "id": "fe8d711f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "573c8dfa"
      },
      "source": [
        "The Binary cross-entropy is given by:\n",
        "\n",
        "$$ J(y^{'}, y) = -y\\log(y^{'}) - (1-y)\\log(1-y{'})$$\n",
        "\n",
        "\n",
        "Derivative of Binary cross-entropy w.r.t its input $y'$.\n",
        "\n",
        "$$\\frac{\\partial J(y',\\;y)}{\\partial y'} = -\\frac{y}{y'} + \\frac{1-y}{1-y'}$$\n",
        "\n"
      ],
      "id": "573c8dfa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T15:18:14.478380Z",
          "start_time": "2021-12-16T15:18:14.460394Z"
        },
        "id": "cddb0390"
      },
      "source": [
        "Now, let's derive the individual derivatives in the Chain rule equations, one by one:<br><br>\n",
        "\n",
        "\n",
        "1) $\\mathbf{\\large{\\frac{\\partial(A_{3})}{\\partial(A_{2})}}} = \\large{\\frac{\\partial J(A_{2},\\;Y)}{\\partial A_{2}}} = \\mathbf{\\large-\\frac{Y}{A_{2}} + \\frac{1\\;-\\;Y}{1\\;-\\;A_{2}}}$ <br><br><br>\n",
        "\n",
        "\n",
        "2) $\\large{\\mathbf{\\frac{\\partial(A_{2})}{\\partial(A_{1})}} = \\frac{\\partial (\\sigma(A_{1}))}{\\partial A_{1}} = \\small{\\mathbf{\\sigma(A_{1}) * (1 - \\sigma(A_{1}))}}}$ <br><br><br>\n",
        "\n",
        "3) $\\large{\\mathbf{\\frac{\\partial (A_{1})}{\\partial (w_{1})}} = \\frac{\\partial (x_{1} * w_{1}\\;+\\;x_{2} * w_{2}\\;+\\;b)}{\\partial (w_{1})} =  \\frac{\\partial (x_{1} * w_{1})}{\\partial(w_{1})} = \\mathbf{x_{1}}}$ <br><br><br>\n",
        "\n",
        "4) $\\large{\\mathbf{\\frac{\\partial (A_{1})}{\\partial (w_{2})}} = \\frac{\\partial (x_{1} * w_{1}\\;+\\;x_{2} * w_{2}\\;+\\;b)}{\\partial (w_{1})} =  \\frac{\\partial (x_{2} * w_{2})}{\\partial(w_{2})} = \\mathbf{x_{2}}}$ <br><br><br>\n",
        "\n",
        "\n",
        "5) $\\large{\\mathbf{\\frac{\\partial (A_{1})}{\\partial (b)}} = \\frac{\\partial (x_{1} * w_{1}\\;+\\;x_{2} * w_{2}\\;+\\;b)}{\\partial (b)} =  \\frac{\\partial (b)}{\\partial(b)} = \\mathbf{1}}$ <br><br><br>"
      ],
      "id": "cddb0390"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de09a9d9"
      },
      "source": [
        "Let's define the constants and variables that we'll be using throughout this example"
      ],
      "id": "de09a9d9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:16.963048Z",
          "start_time": "2021-12-20T16:56:16.937049Z"
        },
        "collapsed": true,
        "id": "e4990d76"
      },
      "outputs": [],
      "source": [
        "# let's first define the constants and variables\n",
        "\n",
        "x1 = tf.constant(1.3, name=\"x1\")\n",
        "x2 = tf.constant(2.1, name=\"x2\")\n",
        "lr = tf.constant(0.1, name=\"learning_rate\")\n",
        "Y  = tf.constant(1.0, name=\"ground_truth\")\n",
        "\n",
        "# ---------\n",
        "w1 = tf.Variable(0.7, name=\"x3\")\n",
        "w2 = tf.Variable(-0.3, name=\"x3\")\n",
        "b  = tf.Variable(1.0, name=\"b\")\n",
        "\n",
        "# Text formatting\n",
        "bold = \"\\033[1m\"\n",
        "end = \"\\033[0m\""
      ],
      "id": "e4990d76"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f4bb0a9"
      },
      "source": [
        "### 4.4 Solving Manually\n",
        "\n",
        "In this section, we define the functions necessary for manually solving the Chain rule."
      ],
      "id": "7f4bb0a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d773eae"
      },
      "source": [
        "Here we are defining the functions for the involved equations, along with their gradient functions.\n",
        "\n",
        "| Method|Notes|\n",
        "|:----|:----|\n",
        "| `wx_plus_b(...)`    |Implements the first equation $A_{1} = w_{1}*x_{1} + w_{2}*x_{2} + b$.|\n",
        "|`grad_wx_plus_b(...)`|Returns the gradients for the variable involved in the above equation during the backward pass.|\n",
        "|`sigmoid(...)`|Function to implement the sigmoid activation function in equation $A_{2}$.|\n",
        "|`grad_sigmoid(...)`|Returns the gradients of `sigmoid` function w.r.t. it's inputs.|\n",
        "|`bce_loss(...)`|Function to implement the binary cross entropy loss function used in equation $A_{3}$.|\n",
        "|`grad_bce_loss(...)`|Returns the gradients of `bce_loss` function w.r.t. it's inputs.|"
      ],
      "id": "9d773eae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:16.995048Z",
          "start_time": "2021-12-20T16:56:16.967027Z"
        },
        "collapsed": true,
        "id": "8797dec8"
      },
      "outputs": [],
      "source": [
        "# Implementation for equation A1:  x1w1 + x2w2 + b and its derivative\n",
        "\n",
        "def wx_plus_b(x1, x2, w1, w2, b):\n",
        "    return x1*w1 + x2* w2 + b\n",
        "\n",
        "\n",
        "# Derivative of WX + B w.r.t. its input W and B\n",
        "def grad_wx_plus_b(x1, x2):\n",
        "    return x1, x2, tf.constant(1.0)\n",
        "\n",
        "# -------------------------------------\n",
        "\n",
        "# Implementation for equation A2: Sigmoid and its derivative\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + tf.math.exp(-x))\n",
        "\n",
        "\n",
        "# Derivative of sigmoid w.r.t. its input.\n",
        "def grad_sigmoid(x):\n",
        "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
        "\n",
        "\n",
        "# -------------------------------------\n",
        "\n",
        "# Implementation for equation A3: Binary cross-entropy and its derivative\n",
        "\n",
        "def bce_loss(y_hat, y):\n",
        "    loss = -(y * tf.math.log(y_hat)) - ((1 - y) * tf.math.log(1.0 - y_hat))\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Derivative of binary cross-entropy w.r.t. its input.\n",
        "def grad_bce_loss(y_hat, y):\n",
        "    return -(y / y_hat) + ((1.0 - y) / (1.0 - y_hat))"
      ],
      "id": "8797dec8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ee79b6d"
      },
      "source": [
        "In the **`forward`** function, the input data is passed through the equations one by one in a forward manner."
      ],
      "id": "4ee79b6d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:17.011048Z",
          "start_time": "2021-12-20T16:56:16.998030Z"
        },
        "collapsed": true,
        "hide_input": false,
        "id": "8616ba07"
      },
      "outputs": [],
      "source": [
        "def forward(x1, x2, w1, w2, b, Y):\n",
        "    A1 = wx_plus_b(x1, x2, w1, w2, b)\n",
        "    A2 = sigmoid(A1)\n",
        "    A3 = bce_loss(A2, Y)\n",
        "    \n",
        "    return_dict = {\n",
        "        \"A1\": A1,\n",
        "        \"A2\": A2,\n",
        "        \"A3\": A3\n",
        "    }\n",
        "    return return_dict"
      ],
      "id": "8616ba07"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c180062f"
      },
      "source": [
        "The **`backward`** function is responsible for calculating the derivative of $A{3}$ w.r.t. the variables $w_{1}, w_{2}, b$.<br>\n",
        "In this function, we first calculate the gradient values for each partial derivative involved in the chain rule equation and then we use them to solve the chain rule."
      ],
      "id": "c180062f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:17.027050Z",
          "start_time": "2021-12-20T16:56:17.015029Z"
        },
        "collapsed": true,
        "id": "a5e5becc"
      },
      "outputs": [],
      "source": [
        "def backward(x1, x2, Y, A1, A2):\n",
        "    \n",
        "    # Compute the gradients of A3 w.r.t  A2 i.e dA3/dA2\n",
        "    d_bce_loss = grad_bce_loss(A2, Y)\n",
        "\n",
        "    # Compute the gradients A2 w.r.t A1 i.e dA2/dA1\n",
        "    d_sigmoid = grad_sigmoid(A1)\n",
        "\n",
        "    # Compute the gradients of weighted sum(z) w.r.t weights and bias\n",
        "    # dA1/dw1, dA1/dw2, dA1/b\n",
        "    d_w1, d_w2, d_b = grad_wx_plus_b(x1, x2)\n",
        "\n",
        "    # Using chain rule to find overall gradient of Loss w.r.t weights and bias\n",
        "    w1_grad = d_bce_loss * d_sigmoid * d_w1\n",
        "    w2_grad = d_bce_loss * d_sigmoid * d_w2\n",
        "    b_grad  = d_bce_loss * d_sigmoid * d_b\n",
        "    \n",
        "    \n",
        "    return_dict = {\n",
        "        \"dA3_dA2\": d_bce_loss,\n",
        "        \"dA2_dA1\": d_sigmoid,\n",
        "        \"dA1_dw1\": d_w1,\n",
        "        \"dA1_dw2\": d_w2,\n",
        "        \"dA1_db\" : d_b,\n",
        "        \"dA3_dw1\": w1_grad,\n",
        "        \"dA3_dw2\": w2_grad,\n",
        "        \"dA3_db\" : b_grad,\n",
        "    }\n",
        "\n",
        "    return return_dict"
      ],
      "id": "a5e5becc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dbd16d0"
      },
      "source": [
        "**Next we will:**\n",
        "\n",
        "1. Execute the `forward` function to get the `initial Loss`.\n",
        "2. Execute the `backward` function to get derivative of `Loss` w.r.t. to the variables.\n",
        "3. Perform the **weight updates**.\n",
        "4. Print the **`New Loss`**. "
      ],
      "id": "2dbd16d0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:17.043048Z",
          "start_time": "2021-12-20T16:56:17.030024Z"
        },
        "id": "35feb6af"
      },
      "outputs": [],
      "source": [
        "# Performing forward pass\n",
        "\n",
        "forward_outputs = forward(x1, x2, w1, w2, b, Y)\n",
        "\n",
        "print(f\"{bold}Forward Pass:{end}\\n\")\n",
        "\n",
        "print(f\"{bold}A1:{end} {forward_outputs['A1'].numpy()}\")\n",
        "print(f\"{bold}A2:{end} {forward_outputs['A2'].numpy()}\")\n",
        "print(f\"{bold}A3:{end} {forward_outputs['A3'].numpy()} <---{bold} Initial Loss{end}\")"
      ],
      "id": "35feb6af"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:17.074048Z",
          "start_time": "2021-12-20T16:56:17.046049Z"
        },
        "id": "cb525ba3"
      },
      "outputs": [],
      "source": [
        "# Performing backward pass\n",
        "\n",
        "A1 = forward_outputs['A1']\n",
        "A2 = forward_outputs['A2']\n",
        "\n",
        "backward_outputs = backward(x1, x2, Y, A1, A2)\n",
        "\n",
        "print(f\"{bold}Backward Pass: Step 1{end}\\n\")\n",
        "print(f\"{bold}Individual Derivatives:{end}\\n\")\n",
        "\n",
        "print(f\"{bold}dA3/dA2{end} = {backward_outputs['dA3_dA2']}\")\n",
        "print(f\"{bold}dA2/dA1{end} = {backward_outputs['dA2_dA1']}\")\n",
        "print(f\"{bold}dA1/dw1{end} = {backward_outputs['dA1_dw1']}\")\n",
        "print(f\"{bold}dA1/dw2{end} = {backward_outputs['dA1_dw2']}\")\n",
        "print(f\"{bold}dA1/db {end} = {backward_outputs['dA1_db']}\")\n",
        "\n",
        "print(\"\\n-----------------\\n\")\n",
        "\n",
        "print(f\"{bold}Gradient of A3 w.r.t. variables:{end}\\n\")\n",
        "\n",
        "print(f\"{bold}dA3/dw1{end} = {backward_outputs['dA3_dw1']}\")\n",
        "print(f\"{bold}dA3/dw2{end} = {backward_outputs['dA3_dw2']}\")\n",
        "print(f\"{bold}dA3/db {end} = {backward_outputs['dA3_db']}\")"
      ],
      "id": "cb525ba3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "437c160b"
      },
      "source": [
        "The **`weight_update`** function applies the weight-update rule to change the weight values during backpropagation. "
      ],
      "id": "437c160b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:17.089049Z",
          "start_time": "2021-12-20T16:56:17.078026Z"
        },
        "collapsed": true,
        "id": "e9184679"
      },
      "outputs": [],
      "source": [
        "def weight_update(w1, w2, b, dw1, dw2, db, lr):\n",
        "    \n",
        "    # w1, w2 and b are objects of tf.Variable class\n",
        "    # They are updated in place\n",
        "    \n",
        "    w1.assign_sub(lr * dw1) # w1 = w1 - lr * dw1\n",
        "    w2.assign_sub(lr * dw2)\n",
        "    b.assign_sub(lr * db)\n",
        "\n",
        "    return w1, w2, b"
      ],
      "id": "e9184679"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:17.120022Z",
          "start_time": "2021-12-20T16:56:17.092049Z"
        },
        "id": "3ff65660"
      },
      "outputs": [],
      "source": [
        "w1_grad = backward_outputs[\"dA3_dw1\"]\n",
        "w2_grad = backward_outputs[\"dA3_dw2\"]\n",
        "b_grad  = backward_outputs[\"dA3_db\"]\n",
        "\n",
        "# keeping a copy of old w and b for comparison\n",
        "# as w and b will be updated inplace\n",
        "\n",
        "w1_old = tf.identity(w1, name=\"old_w1\")\n",
        "w2_old = tf.identity(w2, name=\"old_w2\")\n",
        "b_old  = tf.identity(b,  name=\"old_b\")\n",
        "\n",
        "# Perform Weight Update\n",
        "w1_updated, w2_updated, b_updated = weight_update(w1, w2, b, w1_grad, w2_grad, b_grad, lr)\n",
        "\n",
        "print(f\"{bold}Backward Pass: Step 2{end}\\n\")\n",
        "print(f\"{bold}Parameter Updates{end}\\n\")\n",
        "\n",
        "print(f\"{bold}w1{end} --> {bold}Old:{end} {w1_old.numpy():<20} {bold}New:{end} {w1_updated.numpy()}\")\n",
        "print(f\"{bold}w2{end} --> {bold}Old:{end} {w2_old.numpy():<20} {bold}New:{end} {w2_updated.numpy()}\")\n",
        "print(f\"{bold}b{end}  --> {bold}Old:{end} {b_old.numpy():<19}  {bold}New:{end} {b_updated.numpy()}\")"
      ],
      "id": "3ff65660"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95b489bf"
      },
      "source": [
        "**Comparing the Old and New Loss**"
      ],
      "id": "95b489bf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:17.150021Z",
          "start_time": "2021-12-20T16:56:17.124032Z"
        },
        "id": "e91d468f"
      },
      "outputs": [],
      "source": [
        "# New loss\n",
        "\n",
        "new_forward_outputs = forward(x1, x2, w1_updated, w2_updated, b_updated, Y)\n",
        "\n",
        "old_A3 = forward_outputs[\"A3\"]\n",
        "new_A3 = new_forward_outputs[\"A3\"]\n",
        "\n",
        "# We can also pass w1, w2 and b as the objects are being replaced in-place \n",
        "# _, _, new_loss = forward(x1, x2, w1, w2, b, Y)\n",
        "\n",
        "print(f\"{bold}Checking New Loss{end}:\\n\")\n",
        "\n",
        "print(f\"{bold}LOSS{end} --> {bold}Old:{end} {old_A3.numpy():<20} {bold}New:{end} {new_A3.numpy()}\")"
      ],
      "id": "e91d468f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ed6d82b"
      },
      "source": [
        "### 4.5 Using GradientTape"
      ],
      "id": "0ed6d82b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T17:05:55.292201Z",
          "start_time": "2021-12-20T17:05:55.280212Z"
        },
        "collapsed": true,
        "id": "9631b37c"
      },
      "outputs": [],
      "source": [
        "# Redefine the constants and variables\n",
        "\n",
        "x1 = tf.constant(1.3, name=\"x1\")\n",
        "x2 = tf.constant(2.1, name=\"x2\")\n",
        "lr = tf.constant(0.1, name=\"learning_rate\")\n",
        "Y  = tf.constant(1.0, name=\"ground_truth\")\n",
        "\n",
        "# ---------\n",
        "w1 = tf.Variable(0.7, name=\"x3\")\n",
        "w2 = tf.Variable(-0.3, name=\"x3\")\n",
        "b  = tf.Variable(1.0, name=\"b\")"
      ],
      "id": "9631b37c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b9a6bfa"
      },
      "source": [
        "* Using `GradientTape` to perform automatic differentiaton.\n",
        "* Here, `persistent=True` because we'll be calling the `.gradient(...)` method more than once"
      ],
      "id": "4b9a6bfa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T17:05:55.777192Z",
          "start_time": "2021-12-20T17:05:55.761210Z"
        },
        "collapsed": true,
        "id": "71b8a2c8"
      },
      "outputs": [],
      "source": [
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    # record operations\n",
        "    A1 = w1 * x1 + w2 * x2 + b\n",
        "    A2 = sigmoid(A1)\n",
        "    A3 = bce_loss(A2, Y)"
      ],
      "id": "71b8a2c8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T17:05:56.169210Z",
          "start_time": "2021-12-20T17:05:56.157192Z"
        },
        "id": "52064ac9"
      },
      "outputs": [],
      "source": [
        "print(f\"{bold}Forward Pass:{end}\\n\")\n",
        "\n",
        "print(f\"{bold}A1:{end} {A1.numpy()}\")\n",
        "print(f\"{bold}A2:{end} {A2.numpy()}\")\n",
        "print(f\"{bold}A3:{end} {A3.numpy()} <---{bold} Initial Loss{end}\")"
      ],
      "id": "52064ac9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T17:05:56.656194Z",
          "start_time": "2021-12-20T17:05:56.633197Z"
        },
        "id": "ece9d79c"
      },
      "outputs": [],
      "source": [
        "print(f\"{bold}Backward Pass: Step 1{end}\\n\")\n",
        "print(f\"{bold}Individual Derivatives:{end}\\n\")\n",
        "\n",
        "\n",
        "dA3_dA2 = tape.gradient(A3, A2)\n",
        "\n",
        "dA2_dA1 = tape.gradient(A2, A1)\n",
        "\n",
        "dA1_dw1 = tape.gradient(A1, w1)\n",
        "\n",
        "dA1_dw2 = tape.gradient(A1, w2)\n",
        "\n",
        "dA1_db  = tape.gradient(A1, b)\n",
        "\n",
        "\n",
        "print(f\"{bold}dA3/dA2{end} = {dA3_dA2}\")\n",
        "print(f\"{bold}dA2/dA1{end} = {dA2_dA1}\")\n",
        "print(f\"{bold}dA1/dw1{end} = {dA1_dw1}\")\n",
        "print(f\"{bold}dA1/dw2{end} = {dA1_dw2}\")\n",
        "print(f\"{bold}dA1/db{end} =  {dA1_db}\")\n",
        "\n",
        "print(\"\\n-----------------\\n\")\n",
        "\n",
        "\n",
        "# implementing Chain rule\n",
        "dA3_dw1 = dA3_dA2 * dA2_dA1 * dA1_dw1\n",
        "dA3_dw2 = dA3_dA2 * dA2_dA1 * dA1_dw2\n",
        "dA3_db  = dA3_dA2 * dA2_dA1 * dA1_db\n",
        "\n",
        "print(f\"{bold}Gradient of A3 wrt. variables:{end}\\n\")\n",
        "\n",
        "print(f\"{bold}dA3/dw1{end} = {dA3_dw1}\")\n",
        "print(f\"{bold}dA3/dw2{end} = {dA3_dw2}\")\n",
        "print(f\"{bold}dA3/db{end}  = {dA3_db}\")"
      ],
      "id": "ece9d79c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39e753f5"
      },
      "source": [
        "**You can clearly see that the values calculated manually are the same as those derived via GradientTape.**\n",
        "\n",
        "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />"
      ],
      "id": "39e753f5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f86ddb97"
      },
      "source": [
        "## 5 Direct Implementation using GradientTape\n",
        "\n",
        "There's one more method which is more general and direct that TensorFlow allows us to use: \n",
        "\n",
        "> ***In this, we can directly pass all the variables against which we want to calculate the derivatives. TensorFlow will then internally perform the chain rule and return us the proper answer.***"
      ],
      "id": "f86ddb97"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:17.245025Z",
          "start_time": "2021-12-20T16:56:17.233050Z"
        },
        "collapsed": true,
        "id": "c8a0277e"
      },
      "outputs": [],
      "source": [
        "# Redefine the constants and variables\n",
        "\n",
        "x1 = tf.constant(1.3, name=\"x1\")\n",
        "x2 = tf.constant(2.1, name=\"x2\")\n",
        "lr = tf.constant(0.1, name=\"learning_rate\")\n",
        "Y  = tf.constant(1.0, name=\"ground_truth\")\n",
        "\n",
        "# ---------\n",
        "w1 = tf.Variable(0.7, name=\"x3\")\n",
        "w2 = tf.Variable(-0.3, name=\"x3\")\n",
        "b  = tf.Variable(1.0, name=\"b\")"
      ],
      "id": "c8a0277e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:17.260027Z",
          "start_time": "2021-12-20T16:56:17.248031Z"
        },
        "collapsed": true,
        "id": "2f6e2813"
      },
      "outputs": [],
      "source": [
        "def compute(x1, x2, w1, w2, b, Y):\n",
        "    \n",
        "    # Notice we have not used persistent=True\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        outputs = forward(x1, x2, w1, w2, b, Y)\n",
        "    \n",
        "    # passing all variables with respect to which \n",
        "    # we want to calculate the derivative of A3\n",
        "    grads = tape.gradient(outputs[\"A3\"], [w1, w2, b])\n",
        "    \n",
        "    return outputs, grads"
      ],
      "id": "2f6e2813"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:17.291021Z",
          "start_time": "2021-12-20T16:56:17.263026Z"
        },
        "id": "22f31641"
      },
      "outputs": [],
      "source": [
        "forward_outputs, gradients = compute(x1, x2, w1, w2, b, Y)\n",
        "\n",
        "print(f\"{bold}Backward Pass: Step 1{end}\\n\")\n",
        "print(f\"{bold}Direct Gradient of A3 wrt. variables using GradientTape:{end}\\n\")\n",
        "\n",
        "print(f\"{bold}dA3/dw1{end} = {gradients[0]}\")\n",
        "print(f\"{bold}dA3/dw2{end} = {gradients[1]}\")\n",
        "print(f\"{bold}dA3/db{end}  = {gradients[2]}\")"
      ],
      "id": "22f31641"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:17.307018Z",
          "start_time": "2021-12-20T16:56:17.294025Z"
        },
        "id": "30b595b4"
      },
      "outputs": [],
      "source": [
        "# keeping a copy of old w and b for comparison\n",
        "# as w and b will be updated inplace\n",
        "\n",
        "w1_old = tf.identity(w1, name=\"old_w1\")\n",
        "w2_old = tf.identity(w2, name=\"old_w2\")\n",
        "b_old  = tf.identity(b,  name=\"old_b\")\n",
        "\n",
        "# Perform Weight Update\n",
        "\n",
        "w1_updated, w2_updated, b_updated = weight_update(w1, w2, b, gradients[0], gradients[1], gradients[2], lr)\n",
        "\n",
        "print(f\"{bold}Backward Pass: Step 2{end}\\n\")\n",
        "print(f\"{bold}Parameter Updates{end}\\n\")\n",
        "\n",
        "print(f\"{bold}w1{end} --> {bold}Old:{end} {w1_old.numpy():<20} {bold}New:{end} {w1_updated.numpy()}\")\n",
        "print(f\"{bold}w2{end} --> {bold}Old:{end} {w2_old.numpy():<20} {bold}New:{end} {w2_updated.numpy()}\")\n",
        "print(f\"{bold}b{end}  --> {bold}Old:{end} {b_old.numpy():<19}  {bold}New:{end} {b_updated.numpy()}\")"
      ],
      "id": "30b595b4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-20T16:56:17.323027Z",
          "start_time": "2021-12-20T16:56:17.309021Z"
        },
        "id": "9316c974"
      },
      "outputs": [],
      "source": [
        "# New loss computation\n",
        "\n",
        "new_forward_outputs = forward(x1, x2, w1_updated, w2_updated, b_updated, Y)\n",
        "\n",
        "old_A3 = forward_outputs[\"A3\"]\n",
        "new_A3 = new_forward_outputs[\"A3\"]\n",
        "\n",
        "# We can also pass w1, w2, b due to the objects being replaced in the memory \n",
        "# _, _, new_loss = forward(x1, x2, w1, w2, b, Y)\n",
        "\n",
        "print(f\"{bold}Checking New Loss{end}:\\n\")\n",
        "\n",
        "print(f\"{bold}LOSS{end} --> {bold}Old:{end} {old_A3.numpy():<20} {bold}New:{end} {new_A3.numpy()}\")"
      ],
      "id": "9316c974"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3.8",
      "language": "python",
      "name": "python38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": true,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "number",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": true
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": [],
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "292.679px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}